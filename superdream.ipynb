{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Superdream notebook\n",
    "\n",
    "The code is shamelessy copied from https://github.com/jrosebr1/bat-country/, and adapted to be easy to use with IPython Notebooks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines all the classes, functions and variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from __future__ import print_function\n",
    "from google.protobuf import text_format\n",
    "from cStringIO import StringIO\n",
    "from PIL import Image\n",
    "import PIL.Image\n",
    "from IPython.display import clear_output, Image, display\n",
    "import scipy.ndimage as nd\n",
    "import numpy as np\n",
    "import caffe\n",
    "import os\n",
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()\n",
    "layernames = 'data,conv1/7x7_s2,pool1/3x3_s2,pool1/norm1,conv2/3x3_reduce,conv2/3x3,conv2/norm2,pool2/3x3_s2,pool2/3x3_s2_pool2/3x3_s2_0_split_0,pool2/3x3_s2_pool2/3x3_s2_0_split_1,pool2/3x3_s2_pool2/3x3_s2_0_split_2,pool2/3x3_s2_pool2/3x3_s2_0_split_3,inception_3a/1x1,inception_3a/3x3_reduce,inception_3a/3x3,inception_3a/5x5_reduce,inception_3a/5x5,inception_3a/pool,inception_3a/pool_proj,inception_3a/output,inception_3a/output_inception_3a/output_0_split_0,inception_3a/output_inception_3a/output_0_split_1,inception_3a/output_inception_3a/output_0_split_2,inception_3a/output_inception_3a/output_0_split_3,inception_3b/1x1,inception_3b/3x3_reduce,inception_3b/3x3,inception_3b/5x5_reduce,inception_3b/5x5,inception_3b/pool,inception_3b/pool_proj,inception_3b/output,pool3/3x3_s2,pool3/3x3_s2_pool3/3x3_s2_0_split_0,pool3/3x3_s2_pool3/3x3_s2_0_split_1,pool3/3x3_s2_pool3/3x3_s2_0_split_2,pool3/3x3_s2_pool3/3x3_s2_0_split_3,inception_4a/1x1,inception_4a/3x3_reduce,inception_4a/3x3,inception_4a/5x5_reduce,inception_4a/5x5,inception_4a/pool,inception_4a/pool_proj,inception_4a/output,inception_4a/output_inception_4a/output_0_split_0,inception_4a/output_inception_4a/output_0_split_1,inception_4a/output_inception_4a/output_0_split_2,inception_4a/output_inception_4a/output_0_split_3,inception_4b/1x1,inception_4b/3x3_reduce,inception_4b/3x3,inception_4b/5x5_reduce,inception_4b/5x5,inception_4b/pool,inception_4b/pool_proj,inception_4b/output,inception_4b/output_inception_4b/output_0_split_0,inception_4b/output_inception_4b/output_0_split_1,inception_4b/output_inception_4b/output_0_split_2,inception_4b/output_inception_4b/output_0_split_3,inception_4c/1x1,inception_4c/3x3_reduce,inception_4c/3x3,inception_4c/5x5_reduce,inception_4c/5x5,inception_4c/pool,inception_4c/pool_proj,inception_4c/output,inception_4c/output_inception_4c/output_0_split_0,inception_4c/output_inception_4c/output_0_split_1,inception_4c/output_inception_4c/output_0_split_2,inception_4c/output_inception_4c/output_0_split_3,inception_4d/1x1,inception_4d/3x3_reduce,inception_4d/3x3,inception_4d/5x5_reduce,inception_4d/5x5,inception_4d/pool,inception_4d/pool_proj,inception_4d/output,inception_4d/output_inception_4d/output_0_split_0,inception_4d/output_inception_4d/output_0_split_1,inception_4d/output_inception_4d/output_0_split_2,inception_4d/output_inception_4d/output_0_split_3,inception_4e/1x1,inception_4e/3x3_reduce,inception_4e/3x3,inception_4e/5x5_reduce,inception_4e/5x5,inception_4e/pool,inception_4e/pool_proj,inception_4e/output,pool4/3x3_s2,pool4/3x3_s2_pool4/3x3_s2_0_split_0,pool4/3x3_s2_pool4/3x3_s2_0_split_1,pool4/3x3_s2_pool4/3x3_s2_0_split_2,pool4/3x3_s2_pool4/3x3_s2_0_split_3,inception_5a/1x1,inception_5a/3x3_reduce,inception_5a/3x3,inception_5a/5x5_reduce,inception_5a/5x5,inception_5a/pool,inception_5a/pool_proj,inception_5a/output,inception_5a/output_inception_5a/output_0_split_0,inception_5a/output_inception_5a/output_0_split_1,inception_5a/output_inception_5a/output_0_split_2,inception_5a/output_inception_5a/output_0_split_3,inception_5b/1x1,inception_5b/3x3_reduce,inception_5b/3x3,inception_5b/5x5_reduce,inception_5b/5x5,inception_5b/pool,inception_5b/pool_proj,inception_5b/output,pool5/7x7_s1,loss3/classifier,prob'\n",
    "layernames = layernames.split(\",\")\n",
    "def showarray(a, fmt='jpeg'):\n",
    "    a = np.uint8(np.clip(a, 0, 255))\n",
    "    f = StringIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))\n",
    "\n",
    "class BatCountry:\n",
    "\tdef __init__(self, base_path, deploy_path=None, model_path=None,\n",
    "\t\tpatch_model=\"./tmp.prototxt\", mean=(104.0, 116.0, 122.0),\n",
    "\t\tchannels=(2, 1, 0)):\n",
    "\t\t# if the deploy path is None, set the default\n",
    "\t\tif deploy_path is None:\n",
    "\t\t\tdeploy_path = base_path + \"/deploy.prototxt\"\n",
    "\n",
    "\t\t# if the model path is None, set it to the default GoogleLeNet model\n",
    "\t\tif model_path is None:\n",
    "\t\t\tmodel_path = base_path + \"/bvlc_googlenet.caffemodel\"\n",
    "\n",
    "\t\t# check to see if the model should be patched to compute gradients\n",
    "\t\tif patch_model:\n",
    "\t\t\tmodel = caffe.io.caffe_pb2.NetParameter()\n",
    "\t\t\ttext_format.Merge(open(deploy_path).read(), model)\n",
    "\t\t\tmodel.force_backward = True\n",
    "\t\t\tf = open(patch_model, \"w\")\n",
    "\t\t\tf.write(str(model))\n",
    "\t\t\tf.close()\n",
    "\n",
    "\t\t# load the network and store the patched model path\n",
    "\t\tself.net = caffe.Classifier(patch_model, model_path, mean=np.float32(mean),\n",
    "\t\t\tchannel_swap=channels)\n",
    "\t\tself.patch_model = patch_model\n",
    "\n",
    "\tdef dream(self, image, iter_n=10, octave_n=4, octave_scale=1.4,\n",
    "\t\tend=\"inception_4c/output\", clip=True, step_fn=None, objective_fn=None,\n",
    "\t\tpreprocess_fn=None, deprocess_fn=None, verbose=True, visualize=True,\n",
    "\t\t**step_params):\n",
    "\t\t# if a step function has not been supplied, initialize it as the\n",
    "\t\t# standard gradient ascent step\n",
    "\t\tif step_fn is None:\n",
    "\t\t\tstep_fn = BatCountry.gradient_ascent_step\n",
    "\n",
    "\t\t# if the objective function has not been supplied, initialize it\n",
    "\t\t# as the L2 objective\n",
    "\t\tif objective_fn is None:\n",
    "\t\t\tobjective_fn = BatCountry.L2_objective\n",
    "\n",
    "\t\t# if the preprocess function has not been supplied, initialize it\n",
    "\t\tif preprocess_fn is None:\n",
    "\t\t\tpreprocess_fn = BatCountry.preprocess\n",
    "\n",
    "\t\t# if the deprocess function has not been supplied, initialize it\n",
    "\t\tif deprocess_fn is None:\n",
    "\t\t\tdeprocess_fn = BatCountry.deprocess\n",
    "\n",
    "\t\t# initialize the visualization list\n",
    "\t\tvisualizations = []\n",
    "\n",
    "\t\t# prepare base images for all octaves\n",
    "\t\toctaves = [preprocess_fn(self.net, image)]\n",
    "\n",
    "\t\tfor i in xrange(octave_n - 1):\n",
    "\t\t\toctaves.append(nd.zoom(octaves[-1], (1, 1.0 / octave_scale,\n",
    "\t\t\t\t1.0 / octave_scale), order=1))\n",
    "\n",
    "\t\t# allocate image for network-produced details\n",
    "\t\tdetail = np.zeros_like(octaves[-1])\n",
    "\t\tsrc = self.net.blobs[\"data\"]\n",
    "\n",
    "\t\tfor octave, octave_base in enumerate(octaves[::-1]):\n",
    "\t\t\th, w = octave_base.shape[-2:]\n",
    "\n",
    "\t\t\tif octave > 0:\n",
    "\t\t\t\t# upscale details from the previous octave\n",
    "\t\t\t\th1, w1 = detail.shape[-2:]\n",
    "\t\t\t\tdetail = nd.zoom(detail, (1, 1.0 * h/ h1, 1.0 * w / w1), order=1)\n",
    "\n",
    " \t\t\t# resize the network's input image size\n",
    "\t\t\tsrc.reshape(1, 3, h, w)\n",
    "\t\t\tsrc.data[0] = octave_base + detail\n",
    "\n",
    "\t\t\tfor i in xrange(iter_n):\n",
    "\t\t\t\tstep_fn(self.net, end=end, clip=clip, objective_fn=objective_fn,\n",
    "\t\t\t\t\t**step_params)\n",
    "\n",
    "\t\t\t\t# visualization\n",
    "\t\t\t\tvis = deprocess_fn(self.net, src.data[0])\n",
    "\t\t\t\tshowarray(vis)\n",
    "\t\t\t\t# adjust image contrast if clipping is disabled\n",
    "\t\t\t\tif not clip: \n",
    "\t\t\t\t\tvis = vis * (255.0 / np.percentile(vis, 99.98))\n",
    "\n",
    "\t\t\t\tif verbose:\n",
    "\t\t\t\t\tprint(\"octave={}, iter={}, layer={}, image_dim={}\".format(octave, i, end, vis.shape))\n",
    "\t\t\t\t\tclear_output(wait=True)\n",
    "                    \n",
    "\t\t\t\t# check to see if the visualization list should be\n",
    "\t\t\t\t# updated\n",
    "\t\t\t\tif visualize:\n",
    "\t\t\t\t\tk = \"octave_{}-iter_{}-layer_{}\".format(octave, i,\n",
    "\t\t\t\t\t\tend.replace(\"/\", \"_\"))\n",
    "\t\t\t\t\tvisualizations.append((k, vis))\n",
    "\n",
    "\t\t\t# extract details produced on the current octave\n",
    "\t\t\tdetail = src.data[0] - octave_base\n",
    "\n",
    "\t\t# grab the resulting image\n",
    "\t\tr = deprocess_fn(self.net, src.data[0])\n",
    "\n",
    "\t\t# check to see if the visualizations should be included\n",
    "\t\tif visualize:\n",
    "\t\t\tr = (r, visualizations)\n",
    "\n",
    "\t\treturn r\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef gradient_ascent_step(net, step_size=1.5, end=\"inception_4c/output\",\n",
    "\t\tjitter=128, clip=True, objective_fn=None, **objective_params):\n",
    "\t\t# if the objective function is None, initialize it as\n",
    "\t\t# the standard L2 objective\n",
    "\t\tif objective_fn is None:\n",
    "\t\t\tobjective_fn = BatCountry.L2_objective\n",
    "\n",
    "\t\t# input image is stored in Net's 'data' blob\n",
    "\t\tsrc = net.blobs[\"data\"]\n",
    "\t\tdst = net.blobs[end]\n",
    "\n",
    "\t\t# apply jitter shift\n",
    "\t\tox, oy = np.random.randint(-jitter, jitter + 1, 2)\n",
    "\t\tsrc.data[0] = np.roll(np.roll(src.data[0], ox, -1), oy, -2)\n",
    "\n",
    "\t\tnet.forward(end=end)\n",
    "\t\tobjective_fn(dst, **objective_params)\n",
    "\t\tnet.backward(start=end)\n",
    "\t\tg = src.diff[0]\n",
    "\n",
    "\t\t# apply normalized ascent step to the input image\n",
    "\t\tsrc.data[:] += step_size / np.abs(g).mean() * g\n",
    "\n",
    " \t\t# unshift image\n",
    "\t\tsrc.data[0] = np.roll(np.roll(src.data[0], -ox, -1), -oy, -2)\n",
    "\n",
    "\t\t# unshift image\n",
    "\t\tif clip:\n",
    "\t\t\tbias = net.transformer.mean[\"data\"]\n",
    "\t\t\tsrc.data[:] = np.clip(src.data, -bias, 255 - bias)\n",
    "\n",
    "\tdef layers(self):\n",
    "\t\t# return the layers of the network\n",
    "\t\treturn self.net._layer_names\n",
    "\n",
    "\tdef cleanup(self):\n",
    "\t\t# remove the patched model from disk\n",
    "\t\tos.remove(self.patch_model)\n",
    "\n",
    "\tdef prepare_guide(self, image, end=\"inception_4c/output\", maxW=224, maxH=224,\n",
    "\t\tpreprocess_fn=None):\n",
    "\t\t# if the preprocess function has not been supplied, initialize it\n",
    "\t\tif preprocess_fn is None:\n",
    "\t\t\tpreprocess_fn = BatCountry.preprocess\n",
    "\n",
    "\t\t# grab dimensions of input image\n",
    "\t\t(w, h) = image.size\n",
    "\n",
    "\t\t# GoogLeNet was trained on images with maximum width and heights\n",
    "\t\t# of 224 pixels -- if either dimension is larger than 224 pixels,\n",
    "\t\t# then we'll need to do some resizing\n",
    "\t\tif h > maxH or w > maxW:\n",
    "\t\t\t# resize based on width\n",
    "\t\t\tif w > h:\n",
    "\t\t\t\tr = maxW / float(w)\n",
    "\n",
    "\t\t\t# resize based on height\n",
    "\t\t\telse:\n",
    "\t\t\t\tr = maxH / float(h)\n",
    "\n",
    "\t\t\t# resize the image\n",
    "\t\t\t(nW, nH) = (int(r * w), int(r * h))\n",
    "\t\t\timage = np.float32(image.resize((nW, nH), PIL.Image.BILINEAR))\n",
    "\n",
    "\t\t(src, dst) = (self.net.blobs[\"data\"], self.net.blobs[end])\n",
    "\t\tsrc.reshape(1, 3, nH, nW)\n",
    "\t\tsrc.data[0] = preprocess_fn(self.net, image)\n",
    "\t\tself.net.forward(end=end)\n",
    "\t\tguide_features = dst.data[0].copy()\n",
    "\n",
    "\t\treturn guide_features\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef L2_objective(dst):\n",
    "\t\tdst.diff[:] = dst.data\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef guided_objective(dst, objective_features):\n",
    "\t\tx = dst.data[0].copy()\n",
    "\t\ty = objective_features\n",
    "\t\tch = x.shape[0]\n",
    "\t\tx = x.reshape(ch,-1)\n",
    "\t\ty = y.reshape(ch,-1)\n",
    "\n",
    "\t\t# compute the matrix of dot-products with guide features\n",
    "\t\tA = x.T.dot(y) \n",
    "\n",
    "\t\t# select ones that match best\n",
    "\t\tdst.diff[0].reshape(ch, -1)[:] = y[:,A.argmax(1)]\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef preprocess(net, img):\n",
    "\t\treturn np.float32(np.rollaxis(img, 2)[::-1]) - net.transformer.mean[\"data\"]\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef deprocess(net, img):\n",
    "\t\treturn np.dstack((img + net.transformer.mean[\"data\"])[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the model files.\n",
    "They are not included, so you need to download them. A quick search for the file names should point you in the right direction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = '/Users/spiorf/Desktop/deepdream/models/' # substitute your path here\n",
    "net_fn   = model_path + 'deploy.prototxt'\n",
    "param_fn = model_path + 'bvlc_googlenet.caffemodel'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can begin to play changing the images or the other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc = BatCountry(model_path,deploy_path=net_fn, model_path=param_fn)\n",
    "end=layernames[80]\n",
    "\n",
    "img = np.float32(PIL.Image.open('/Users/spiorf/Desktop/deepdream/source_images/wall720.jpg'))\n",
    "features = bc.prepare_guide(PIL.Image.open('/Users/spiorf/Desktop/deepdream/guide_images/pines.jpg'), end=end)\n",
    "\n",
    "#image = bc.dream(img, iter_n=20, octave_n=4, octave_scale=1.3, end=end, verbose=True, visualize=False)\n",
    "image = bc.dream(img, iter_n=50, octave_n=28, octave_scale=1.1, end=end, objective_fn=BatCountry.guided_objective, \n",
    "                 objective_features=features, verbose=True, visualize=False)\n",
    "bc.cleanup()\n",
    "showarray(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this cell we scan a folder and process every file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rootdir = 'C:/Users/spiorf/Desktop/todream'\n",
    "name_prefix = '_out_'\n",
    "bc = BatCountry(model_path,deploy_path=net_fn, model_path=param_fn)\n",
    "end=layernames[80]\n",
    "\n",
    "features = bc.prepare_guide(PIL.Image.open('/Users/spiorf/Desktop/deepdream/guide_images/spider.jpg'), end=end)\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        print(os.path.join(subdir, file))\n",
    "        img = np.float32(PIL.Image.open(os.path.join(subdir, file)))\n",
    "        _=image = bc.dream(img, iter_n=10, octave_n=4, octave_scale=1.3, end=end, objective_fn=BatCountry.guided_objective, objective_features=features, verbose=True, visualize=False)\n",
    "        showarray(_)\n",
    "        fileout = name_prefix + file \n",
    "        PIL.Image.fromarray(np.uint8(_)).save(os.path.join(subdir, fileout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell generates a file for every layer from the same starting image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Output dir\n",
    "rootdir = '/Users/spiorf/Desktop/dreams/'\n",
    "name_prefix = 'layer'\n",
    "\n",
    "if not os.path.exists(rootdir):\n",
    "    os.makedirs(rootdir)\n",
    "    #Start image    \n",
    "startimage = '/Users/spiorf/Desktop/deepdream/source_images/wall720.jpg'\n",
    "\n",
    "\n",
    "    #Guide image\n",
    "guideimage = '/Users/spiorf/Desktop/deepdream/guide_images/pines.jpg'\n",
    "\n",
    "\n",
    "#Init model\n",
    "bc = BatCountry(model_path,deploy_path=net_fn, model_path=param_fn)\n",
    "\n",
    "\n",
    "i=10\n",
    "\n",
    "while i< (len(layernames)):\n",
    "    try:\n",
    "            features = bc.prepare_guide(PIL.Image.open(guideimage), end=layernames[i])\n",
    "            img = np.float32(PIL.Image.open(startimage))\n",
    "#            frame = bc.dream(img, iter_n=20, octave_n=4, octave_scale=1.3, end=layernames[i], verbose=True, visualize=False) # Uncomment for unguided image\n",
    "            frame = bc.dream(img, iter_n=20, octave_n=4, octave_scale=1.4, end=layernames[i],         #\n",
    "                             objective_fn=BatCountry.guided_objective, objective_features=features,   # Comment for unguided image\n",
    "                             verbose=True, visualize=False)                                           #\n",
    "            filename = name_prefix + `i` + \"-\" + layernames[i].replace('/', '')\n",
    "            PIL.Image.fromarray(np.uint8(frame)).save(rootdir + \"%s.jpg\"%filename)\n",
    "            print(i+1, str(layernames[i]))\n",
    "            i +=1\n",
    "    except ValueError:\n",
    "        print('Skipped', i, str(layernames[i]))\n",
    "        i +=1\n",
    "        pass\n",
    "    except KeyError:\n",
    "        print('Skipped', i, str(layernames[i]))\n",
    "        i +=1\n",
    "        pass\n",
    "bc.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell generates a video (in the form of a serie of jpgs) zooming a bit and reprocessing the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Output dir\n",
    "rootdir = '/Users/spiorf/Desktop/dreams/'\n",
    "name_prefix = 'video'\n",
    "\n",
    "if not os.path.exists(rootdir):\n",
    "    os.makedirs(rootdir)\n",
    "    \n",
    "\n",
    "#Start image    \n",
    "startimage = '/Users/spiorf/Desktop/deepdream/source_images/noise3.jpg'\n",
    "\n",
    "\n",
    "#Guide image\n",
    "guideimage = '/Users/spiorf/Desktop/deepdream/guide_images/pines.jpg'\n",
    "\n",
    "#Zoom\n",
    "s = 0.05\n",
    "\n",
    "end=layernames[80]\n",
    "\n",
    "\n",
    "#Init model\n",
    "bc = BatCountry(model_path,deploy_path=net_fn, model_path=param_fn)\n",
    "img = np.float32(PIL.Image.open(startimage))\n",
    "features = bc.prepare_guide(PIL.Image.open(guideimage), end=end)\n",
    "if not os.path.exists(rootdir):\n",
    "    os.makedirs(rootdir)\n",
    "i=1 \n",
    "h, w = frame.shape[:2]\n",
    "\n",
    "frame = img\n",
    "while i< 10000:\n",
    "    try:\n",
    "\n",
    "#            frame = bc.dream(img, iter_n=20, octave_n=4, octave_scale=1.3, end=layernames[i], verbose=True, visualize=False) # Uncomment for unguided image\n",
    "            frame = bc.dream(frame, iter_n=20, octave_n=4, octave_scale=1.4, end=end,                  #\n",
    "                             objective_fn=BatCountry.guided_objective, objective_features=features,   # Comment for unguided image\n",
    "                             verbose=True, visualize=False)                                           #\n",
    "            filename = name_prefix + `i` + \"-\" + layernames[i].replace('/', '')\n",
    "            PIL.Image.fromarray(np.uint8(frame)).save(rootdir + name_prefix + \"%07d.jpg\"%i)\n",
    "            frame = nd.affine_transform(frame, [1-s,1-s,1], [h*s/2,w*s/2,0], order=1)\n",
    "            print(j, str(layernames[i]))\n",
    "            i +=1\n",
    "    except ValueError:\n",
    "        print('Skipped', i, str(layernames[i]))\n",
    "        i +=1\n",
    "        pass\n",
    "    except KeyError:\n",
    "        print('Skipped', i, str(layernames[i]))\n",
    "        i +=1\n",
    "        pass\n",
    "\n",
    "bc.cleanup()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0000 data\n",
    "0001 conv1/7x7_s2\n",
    "0002 pool1/3x3_s2\n",
    "0003 pool1/norm1\n",
    "0004 conv2/3x3_reduce\n",
    "0005 conv2/3x3\n",
    "0006 conv2/norm2\n",
    "0007 pool2/3x3_s2\n",
    "0008 pool2/3x3_s2_pool2/3x3_s2_0_split_0\n",
    "0009 pool2/3x3_s2_pool2/3x3_s2_0_split_1\n",
    "0010 pool2/3x3_s2_pool2/3x3_s2_0_split_2\n",
    "0011 pool2/3x3_s2_pool2/3x3_s2_0_split_3\n",
    "0012 inception_3a/1x1\n",
    "0013 inception_3a/3x3_reduce\n",
    "0014 inception_3a/3x3\n",
    "0015 inception_3a/5x5_reduce\n",
    "0016 inception_3a/5x5\n",
    "0017 inception_3a/pool\n",
    "0018 inception_3a/pool_proj\n",
    "0019 inception_3a/output\n",
    "0020 inception_3a/output_inception_3a/output_0_split_0\n",
    "0021 inception_3a/output_inception_3a/output_0_split_1\n",
    "0022 inception_3a/output_inception_3a/output_0_split_2\n",
    "0023 inception_3a/output_inception_3a/output_0_split_3\n",
    "0024 inception_3b/1x1\n",
    "0025 inception_3b/3x3_reduce\n",
    "0026 inception_3b/3x3\n",
    "0027 inception_3b/5x5_reduce\n",
    "0028 inception_3b/5x5\n",
    "0029 inception_3b/pool\n",
    "0030 inception_3b/pool_proj\n",
    "0031 inception_3b/output\n",
    "0032 pool3/3x3_s2\n",
    "0033 pool3/3x3_s2_pool3/3x3_s2_0_split_0\n",
    "0034 pool3/3x3_s2_pool3/3x3_s2_0_split_1\n",
    "0035 pool3/3x3_s2_pool3/3x3_s2_0_split_2\n",
    "0036 pool3/3x3_s2_pool3/3x3_s2_0_split_3\n",
    "0037 inception_4a/1x1\n",
    "0038 inception_4a/3x3_reduce\n",
    "0039 inception_4a/3x3\n",
    "0040 inception_4a/5x5_reduce\n",
    "0041 inception_4a/5x5\n",
    "0042 inception_4a/pool\n",
    "0043 inception_4a/pool_proj\n",
    "0044 inception_4a/output\n",
    "0045 inception_4a/output_inception_4a/output_0_split_0\n",
    "0046 inception_4a/output_inception_4a/output_0_split_1\n",
    "0047 inception_4a/output_inception_4a/output_0_split_2\n",
    "0048 inception_4a/output_inception_4a/output_0_split_3\n",
    "0049 inception_4b/1x1\n",
    "0050 inception_4b/3x3_reduce\n",
    "0051 inception_4b/3x3\n",
    "0052 inception_4b/5x5_reduce\n",
    "0053 inception_4b/5x5\n",
    "0054 inception_4b/pool\n",
    "0055 inception_4b/pool_proj\n",
    "0056 inception_4b/output\n",
    "0057 inception_4b/output_inception_4b/output_0_split_0\n",
    "0058 inception_4b/output_inception_4b/output_0_split_1\n",
    "0059 inception_4b/output_inception_4b/output_0_split_2\n",
    "0060 inception_4b/output_inception_4b/output_0_split_3\n",
    "0061 inception_4c/1x1\n",
    "0062 inception_4c/3x3_reduce\n",
    "0063 inception_4c/3x3\n",
    "0064 inception_4c/5x5_reduce\n",
    "0065 inception_4c/5x5\n",
    "0066 inception_4c/pool\n",
    "0067 inception_4c/pool_proj\n",
    "0068 inception_4c/output\n",
    "0069 inception_4c/output_inception_4c/output_0_split_0\n",
    "0070 inception_4c/output_inception_4c/output_0_split_1\n",
    "0071 inception_4c/output_inception_4c/output_0_split_2\n",
    "0072 inception_4c/output_inception_4c/output_0_split_3\n",
    "0073 inception_4d/1x1\n",
    "0074 inception_4d/3x3_reduce\n",
    "0075 inception_4d/3x3\n",
    "0076 inception_4d/5x5_reduce\n",
    "0077 inception_4d/5x5\n",
    "0078 inception_4d/pool\n",
    "0079 inception_4d/pool_proj\n",
    "0080 inception_4d/output\n",
    "0081 inception_4d/output_inception_4d/output_0_split_0\n",
    "0082 inception_4d/output_inception_4d/output_0_split_1\n",
    "0083 inception_4d/output_inception_4d/output_0_split_2\n",
    "0084 inception_4d/output_inception_4d/output_0_split_3\n",
    "0085 inception_4e/1x1\n",
    "0086 inception_4e/3x3_reduce\n",
    "0087 inception_4e/3x3\n",
    "0088 inception_4e/5x5_reduce\n",
    "0089 inception_4e/5x5\n",
    "0090 inception_4e/pool\n",
    "0091 inception_4e/pool_proj\n",
    "0092 inception_4e/output\n",
    "0093 pool4/3x3_s2\n",
    "0094 pool4/3x3_s2_pool4/3x3_s2_0_split_0\n",
    "0095 pool4/3x3_s2_pool4/3x3_s2_0_split_1\n",
    "0096 pool4/3x3_s2_pool4/3x3_s2_0_split_2\n",
    "0097 pool4/3x3_s2_pool4/3x3_s2_0_split_3\n",
    "0098 inception_5a/1x1\n",
    "0099 inception_5a/3x3_reduce\n",
    "0100 inception_5a/3x3\n",
    "0101 inception_5a/5x5_reduce\n",
    "0102 inception_5a/5x5\n",
    "0103 inception_5a/pool\n",
    "0104 inception_5a/pool_proj\n",
    "0105 inception_5a/output\n",
    "0106 inception_5a/output_inception_5a/output_0_split_0\n",
    "0107 inception_5a/output_inception_5a/output_0_split_1\n",
    "0108 inception_5a/output_inception_5a/output_0_split_2\n",
    "0109 inception_5a/output_inception_5a/output_0_split_3\n",
    "0110 inception_5b/1x1\n",
    "0111 inception_5b/3x3_reduce\n",
    "0112 inception_5b/3x3\n",
    "0113 inception_5b/5x5_reduce\n",
    "0114 inception_5b/5x5\n",
    "0115 inception_5b/pool\n",
    "0116 inception_5b/pool_proj\n",
    "0117 inception_5b/output\n",
    "0118 pool5/7x7_s1\n",
    "0119 loss3/classifier\n",
    "0120 prob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
